---
title: "Counting the number of logs per day of observation"
author: "Laura Symul"
date: "`r format(Sys.time(), '%d %B, %Y')`"
output: 
  bookdown::html_document2: 
    theme: flatly
    highlight: haddock
    toc: yes
    toc_float: true
    toc_depth: 5
    number_sections: true
    fig_caption: true
---



```{r data_prep_n_logs librairies and stuff, include = FALSE, eval = TRUE}
source("Scripts/00_setup.R")
```


```{r data_prep_n_logs setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, cache = TRUE)
options(scipen=999)
```



Checking for duplicates in the original dataset that Clue provided.

```{r data_prep_cycle_id MISC checking if there are duplicates in the original data, eval = FALSE}

# suspecting for example a duplicate on light period for user # 00147343bfdb6aff66efe58308b606d20959d8c4 on the 2017-06-04

if(par$data_type != "synthetic"){
files=  list.files(paste0(IO$tmp_data, "tracking_raw_batches/"))
j = grep("_batch_1.Rdata", files)
files = files[j]

for(file in files){
  cat(file,"\n")
  load(file = paste0(IO$tmp_data, 'tracking_raw_batches/',file), verbose = TRUE)
  print(which(days$user_id == "00147343bfdb6aff66efe58308b606d20959d8c4"))
}

load(file = paste0(IO$input_data, 'tracking_filtered/tracking_filtered_split_cq.Rdata'))
which(tracking$user_id == "00147343bfdb6aff66efe58308b606d20959d8c4")

TRACKING = tracking[which(tracking$user_id == "00147343bfdb6aff66efe58308b606d20959d8c4"),]

load(file = paste0(IO$input_data, 'tracking_filtered/tracking_filtered_split_cr.Rdata'))
which(tracking$user_id == "00147343bfdb6aff66efe58308b606d20959d8c4")


TRACKING = rbind(TRACKING, tracking[which(tracking$user_id == "00147343bfdb6aff66efe58308b606d20959d8c4"),])
TRACKING[TRACKING$date == "2017-06-04",]

rm(TRACKING, tracking, files, file, j)
}

```


## First we need to remove the duplicated rows in the tracking tables

```{r data_prep_n_logs remove duplicated rows function}

remove_duplicates = function(filename = filenames[1]){
  cat(filename,"\n")
  load(paste0(input_days_folder, filename), verbose = TRUE)
  
  
  # defining a unique identifier for each day of observation (user_id + date)
  days$day_id = paste0(days$user_id, "_", days$date)
  cat(filename, " : day_id created \n")
  
  
  # defining a unique key to check for duplicates
  days$key = paste0(days$day_id, "_",days$category, "_", days$type)
  
  # removing duplicated rows
  d = duplicated(days$key)
  
  if(sum(d)>0){days = days[-which(d),]}
  cat(filename, " : days table : ",sum(d)," duplicated rows removed \n")
  
  days = days[,-which(colnames(days) == "key")]
  
  save(days, file = paste0(output_days_folder, filename))
  cat(filename, " : days table saved \n")
}

```



```{r data_prep_n_logs remove duplicated rows}

#input
input_days_folder = paste0(IO$tmp_data,"days_after_data_prep_format/")
filenames = list.files(input_days_folder)

#output
output_days_folder = paste0(IO$tmp_data,"days_duplicated_rows_removed/")
if(dir.exists(output_days_folder)){unlink(output_days_folder, recursive = TRUE)}
dir.create(output_days_folder)

# parallel processing
tic()
cl = makeCluster(min(length(filenames),par$n_cores), outfile="")
registerDoParallel(cl)
foreach(filename = filenames) %dopar% {remove_duplicates(filename = filename)}
stopCluster(cl)
tac(chunck_name = "data_prep_n_logs remove duplicated rows")

```



```{r data_prep_n_logs remove duplicated rows copy to output folder}
copy_days_tmp2out(output_days_folder)
```



## Counting the number of logs per day of observation (and adding these rows to the tables)


```{r data_prep_n_logs counting and saving n_logs}
tic()

#input
input_days_folder = paste0(IO$tmp_data,"days_duplicated_rows_removed/")
filenames = list.files(input_days_folder)

#output
output_days_folder = paste0(IO$tmp_data,"days_with_n_logs/")
if(dir.exists(output_days_folder)){unlink(output_days_folder, recursive = TRUE)}
dir.create(output_days_folder)



cl = makeCluster(min(length(filenames),par$n_cores), outfile="")
registerDoParallel(cl)

foreach(filename = filenames) %dopar%
{
  cat(filename,"\n")
  load(paste0(input_days_folder, filename), verbose = TRUE)
  

  # counting the number of logs on that day
  days_n_logs = aggregate(type ~ day_id, days, lu)
  # formating the dataframe
  days_n_logs$number = days_n_logs$type
  days_n_logs$type = "n_logs"
  days_n_logs$category = "n_logs"
  
  # making sure the new dataframe with the n_logs has the same # of columns and fill the missing ones with the same value as in the original table
  m = match(days_n_logs$day_id, days$day_id)
  cns = setdiff(colnames(days), colnames(days_n_logs))
  for(cn in cns){eval(parse(text = paste0("days_n_logs$",cn," = days$",cn,"[m]")))}
  rm(m, cn, cns)
  # and re-ordering the columns to make sure they match
  oc = match(colnames(days),colnames(days_n_logs))
  days_n_logs = days_n_logs[,oc]; rm(oc)
  
  # adding the n_logs
  days = rbind(days, days_n_logs)
  # sorting days
  o = order(days$user_id, days$date)
  days = days[o,]; rm(o)
  
  save(days, file = paste0(output_days_folder, filename))
}

stopImplicitCluster()
tac()
```

```{r data_prep_n_logs counting and saving n_logs copying to output folder}
copy_days_tmp2out(output_days_folder)
```



```{r data_prep_n_logs checking counting and saving n_logs}
filenames = list.files(output_days_folder)
load(paste0(output_days_folder, filenames[length(filenames)]), verbose = TRUE)
summary(days)
head(days)
```

